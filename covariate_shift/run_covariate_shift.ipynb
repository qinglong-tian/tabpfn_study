{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86725d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from xgboost import XGBRegressor\n",
    "# NEW: TabPFN regressor\n",
    "from tabpfn import TabPFNRegressor\n",
    "\n",
    "# ============================================================\n",
    "# Helper: training and evaluation for one split (existing methods)\n",
    "# ============================================================\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, rho=0.5, beta=2):\n",
    "    n = len(X_train)\n",
    "    n1 = int((1 - rho) * n)\n",
    "    X_train_1, y_train_1 = X_train[:n1], y_train[:n1]\n",
    "    X_train_2, y_train_2 = X_train[n1:], y_train[n1:]\n",
    "\n",
    "    # pseudo-labeling\n",
    "    lbd_tilde = 0.1 / n\n",
    "    krr_tilde = KernelRidge(kernel=\"rbf\", alpha=lbd_tilde)\n",
    "    krr_tilde.fit(X_train_2, y_train_2)\n",
    "    y_tilde = krr_tilde.predict(X_test)\n",
    "\n",
    "    # lambda grid\n",
    "    lbd_min, lbd_max = 0.1 / n, 1\n",
    "    m = int(np.ceil(np.log(lbd_max / lbd_min) / np.log(beta))) + 1\n",
    "    Lambda = lbd_min * (beta ** np.arange(m))\n",
    "\n",
    "    Alpha = np.zeros((m, n1))\n",
    "    err_est_naive = np.zeros(m)\n",
    "    err_est_pseudo = np.zeros(m)\n",
    "    err_est_real = np.zeros(m)\n",
    "\n",
    "    for j, lbd in enumerate(Lambda):\n",
    "        krr = KernelRidge(kernel=\"rbf\", alpha=lbd)\n",
    "        krr.fit(X_train_1, y_train_1)\n",
    "        Alpha[j] = krr.dual_coef_\n",
    "\n",
    "        err_est_naive[j] = np.mean((krr.predict(X_train_2) - y_train_2) ** 2)\n",
    "        y_lbd = krr.predict(X_test)\n",
    "        err_est_pseudo[j] = np.mean((y_lbd - y_tilde) ** 2)\n",
    "        err_est_real[j] = np.mean((y_lbd - y_test) ** 2)\n",
    "\n",
    "    j_naive, j_pseudo, j_real = map(np.argmin, [err_est_naive, err_est_pseudo, err_est_real])\n",
    "    lbd_naive, lbd_pseudo, lbd_real = Lambda[j_naive], Lambda[j_pseudo], Lambda[j_real]\n",
    "\n",
    "    K = pairwise_kernels(X_test, X_train_1, metric=\"rbf\")\n",
    "    y_new_naive = K @ Alpha[j_naive]\n",
    "    y_new_pseudo = K @ Alpha[j_pseudo]\n",
    "    y_new_real = K @ Alpha[j_real]\n",
    "\n",
    "    def mse(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    return {\n",
    "        \"naive\": mse(y_test, y_new_naive),\n",
    "        \"pseudo\": mse(y_test, y_new_pseudo),\n",
    "        \"real\": mse(y_test, y_new_real),\n",
    "        \"lbd_naive\": lbd_naive,\n",
    "        \"lbd_pseudo\": lbd_pseudo,\n",
    "        \"lbd_real\": lbd_real,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# XGBoost with simple cross-validation (existing)\n",
    "# ============================================================\n",
    "def fit_xgboost(X_train, y_train, X_test, y_test, random_state=0):\n",
    "    \"\"\"\n",
    "    Train XGBoost on (X_train, y_train) with a small CV grid.\n",
    "    Evaluate MSE on X_test/y_test and return (mse, best_params).\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "    }\n",
    "    base = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    gs = GridSearchCV(\n",
    "        estimator=base,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_model = gs.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = float(np.mean((y_test - y_pred) ** 2))\n",
    "    return mse, gs.best_params_\n",
    "\n",
    "# ============================================================\n",
    "# XGBoost (importance-weighted evaluation) reusing tuned params (existing)\n",
    "# ============================================================\n",
    "def xgboost_importance_weighted(X_train, y_train, X_test, y_test, weights, best_params, random_state=0):\n",
    "    \"\"\"\n",
    "    Refit XGBoost with the best params from the base XGB CV and compute\n",
    "    importance-weighted MSE on the test set using 'weights'.\n",
    "    \"\"\"\n",
    "    model = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        eval_metric=\"rmse\",\n",
    "        **best_params\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    iw_mse = float(np.average((y_test - y_pred) ** 2, weights=weights))\n",
    "    return iw_mse\n",
    "\n",
    "# ============================================================\n",
    "# NEW: TabPFN (regression) — train on all X_train, evaluate on shifted test\n",
    "# ============================================================\n",
    "def fit_tabpfn_regressor(X_train, y_train, X_test, y_test, random_state=0, device=None):\n",
    "    \"\"\"\n",
    "    Train TabPFNRegressor on (X_train, y_train) and compute MSE on X_test/y_test.\n",
    "    device: None | 'cpu' | 'cuda' (if GPU is available). Defaults to library’s behavior.\n",
    "    \"\"\"\n",
    "    kwargs = {}\n",
    "    if device is not None:\n",
    "        kwargs[\"device\"] = device\n",
    "\n",
    "    model = TabPFNRegressor(random_state=random_state, **kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = float(np.mean((y_test - y_pred) ** 2))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "483ae330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Airfoil dataset: 1503 samples, 5 features\n",
      "Iteration 1/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 2/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 3/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 4/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 5/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 6/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 7/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 8/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 9/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 10/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "\n",
      "=== Average Results over 10 splits ===\n",
      "Real    : 12.9679 ± 1.7173\n",
      "Pseudo  : 14.9957 ± 1.9942\n",
      "Naive   : 17.2327 ± 2.6326\n",
      "Xgb     : 10.3357 ± 3.0652\n",
      "Xgb_iw  : 21.9676 ± 7.1759\n",
      "Tabpfn  : 2.8241 ± 0.6042\n",
      "\n",
      "Average selected λ (KRR):\n",
      "λ* Naive : 0.002462\n",
      "λ* Pseudo: 0.02011\n",
      "λ* Real  : 0.01046\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load and preprocess data\n",
    "# ============================================================\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat\"\n",
    "colnames = [\n",
    "    \"Frequency\",\n",
    "    \"Angle_of_attack\",\n",
    "    \"Chord_length\",\n",
    "    \"Free_stream_velocity\",\n",
    "    \"Suction_side_displacement_thickness\",\n",
    "    \"Sound_pressure_level\",\n",
    "]\n",
    "df = pd.read_csv(url, sep=\"\\t\", names=colnames)\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "print(f\"Loaded Airfoil dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "# ============================================================\n",
    "# Repeated experiment (10 random splits)\n",
    "# ============================================================\n",
    "rho, beta = 0.5, 2\n",
    "n_repeats = 10\n",
    "results = []\n",
    "\n",
    "for i in range(n_repeats):\n",
    "    # split + scale\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42 + i\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # covariate shift (same as before)\n",
    "    beta1 = np.array([-1, 1, -1, 1, -1])\n",
    "    weights = np.exp(X_test @ beta1)\n",
    "    weights /= np.sum(weights)\n",
    "    idx_shift = np.random.choice(len(X_test), size=len(X_test), replace=True, p=weights)\n",
    "    X_test_shift, y_test_shift = X_test[idx_shift], y_test[idx_shift]\n",
    "    weights_shift = weights[idx_shift]  # align weights to the shifted test set\n",
    "\n",
    "    # existing KRR-based methods\n",
    "    res = train_and_evaluate(X_train, y_train, X_test_shift, y_test_shift, rho=rho, beta=beta)\n",
    "\n",
    "    # existing: XGBoost CV (unweighted)\n",
    "    xgb_mse, xgb_best = fit_xgboost(X_train, y_train, X_test_shift, y_test_shift, random_state=42 + i)\n",
    "    res[\"xgb\"] = xgb_mse\n",
    "    res[\"xgb_best_params\"] = xgb_best\n",
    "\n",
    "    # existing: XGBoost (importance-weighted evaluation)\n",
    "    xgb_iw_mse = xgboost_importance_weighted(\n",
    "        X_train, y_train, X_test_shift, y_test_shift, weights_shift, best_params=xgb_best, random_state=42 + i\n",
    "    )\n",
    "    res[\"xgb_iw\"] = xgb_iw_mse\n",
    "\n",
    "    # NEW: TabPFN regressor (train on all X_train, evaluate on shifted test)\n",
    "    tabpfn_mse = fit_tabpfn_regressor(\n",
    "        X_train, y_train, X_test_shift, y_test_shift, random_state=42 + i, device=None  # set to 'cuda' if available\n",
    "    )\n",
    "    res[\"tabpfn\"] = tabpfn_mse\n",
    "\n",
    "    results.append(res)\n",
    "    print(f\"Iteration {i+1}/10 done. XGB best params: {xgb_best}\")\n",
    "\n",
    "# ============================================================\n",
    "# Aggregate results\n",
    "# ============================================================\n",
    "methods = [\"real\", \"pseudo\", \"naive\", \"xgb\", \"xgb_iw\", \"tabpfn\"]\n",
    "avg_results = {}\n",
    "for method in methods:\n",
    "    vals = [r[method] for r in results]\n",
    "    mean = np.mean(vals)\n",
    "    ste = np.std(vals, ddof=1) / np.sqrt(len(vals))\n",
    "    avg_results[method] = (mean, ste)\n",
    "\n",
    "print(\"\\n=== Average Results over 10 splits ===\")\n",
    "for method in methods:\n",
    "    mean, ste = avg_results[method]\n",
    "    print(f\"{method.capitalize():<8}: {mean:.4f} ± {ste:.4f}\")\n",
    "\n",
    "# (Optional) average selected λ for KRR-based selectors\n",
    "lbd_naive_mean = np.mean([r[\"lbd_naive\"] for r in results])\n",
    "lbd_pseudo_mean = np.mean([r[\"lbd_pseudo\"] for r in results])\n",
    "lbd_real_mean = np.mean([r[\"lbd_real\"] for r in results])\n",
    "print(\"\\nAverage selected λ (KRR):\")\n",
    "print(f\"λ* Naive : {lbd_naive_mean:.4g}\")\n",
    "print(f\"λ* Pseudo: {lbd_pseudo_mean:.4g}\")\n",
    "print(f\"λ* Real  : {lbd_real_mean:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b11a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column names:\n",
      "['Cement (component 1)(kg in a m^3 mixture)', 'Blast Furnace Slag (component 2)(kg in a m^3 mixture)', 'Fly Ash (component 3)(kg in a m^3 mixture)', 'Water  (component 4)(kg in a m^3 mixture)', 'Superplasticizer (component 5)(kg in a m^3 mixture)', 'Coarse Aggregate  (component 6)(kg in a m^3 mixture)', 'Fine Aggregate (component 7)(kg in a m^3 mixture)', 'Age (day)', 'Concrete compressive strength(MPa, megapascals) ']\n",
      "   cement   slag  fly_ash  water  superplasticizer  coarse_agg  fine_agg  age  \\\n",
      "0   540.0    0.0      0.0  162.0               2.5      1040.0     676.0   28   \n",
      "1   540.0    0.0      0.0  162.0               2.5      1055.0     676.0   28   \n",
      "2   332.5  142.5      0.0  228.0               0.0       932.0     594.0  270   \n",
      "3   332.5  142.5      0.0  228.0               0.0       932.0     594.0  365   \n",
      "4   198.6  132.4      0.0  192.0               0.0       978.4     825.5  360   \n",
      "\n",
      "    strength  \n",
      "0  79.986111  \n",
      "1  61.887366  \n",
      "2  40.269535  \n",
      "3  41.052780  \n",
      "4  44.296075  \n",
      "\n",
      "Data shape: (1030, 9)\n"
     ]
    }
   ],
   "source": [
    "# Try another data: Concrete Compressive Strength\n",
    "\n",
    "path = \"./Concrete_Data.xls\"\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "print(\"Original column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df.columns = [\n",
    "    \"cement\",          # Cement (kg in m3)\n",
    "    \"slag\",            # Blast Furnace Slag\n",
    "    \"fly_ash\",         # Fly Ash\n",
    "    \"water\",           # Water\n",
    "    \"superplasticizer\",# Superplasticizer\n",
    "    \"coarse_agg\",      # Coarse Aggregate\n",
    "    \"fine_agg\",        # Fine Aggregate\n",
    "    \"age\",             # Age (days)\n",
    "    \"strength\"         # Concrete compressive strength (MPa)\n",
    "]\n",
    "\n",
    "X = df.drop(columns=[\"strength\"])\n",
    "y = df[\"strength\"]\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nData shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f2e3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Concrete Strength dataset: 1030 samples, 8 features\n"
     ]
    }
   ],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "print(f\"Loaded Concrete Strength dataset: {X.shape[0]} samples, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44374eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 2/10 done. XGB best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 3/10 done. XGB best params: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 4/10 done. XGB best params: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 400, 'subsample': 0.8}\n",
      "Iteration 5/10 done. XGB best params: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_iw\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m xgb_iw_mse\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# NEW: TabPFN regressor (train on all X_train, evaluate on shifted test)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m tabpfn_mse \u001b[38;5;241m=\u001b[39m fit_tabpfn_regressor(\n\u001b[1;32m     38\u001b[0m     X_train, y_train, X_test_shift, y_test_shift, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m \u001b[38;5;241m+\u001b[39m i, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# set to 'cuda' if available\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtabpfn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tabpfn_mse\n\u001b[1;32m     42\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "Cell \u001b[0;32mIn[22], line 138\u001b[0m, in \u001b[0;36mfit_tabpfn_regressor\u001b[0;34m(X_train, y_train, X_test, y_test, random_state, device)\u001b[0m\n\u001b[1;32m    136\u001b[0m model \u001b[38;5;241m=\u001b[39m TabPFNRegressor(random_state\u001b[38;5;241m=\u001b[39mrandom_state, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m--> 138\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    139\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean((y_test \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mse\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/regressor.py:624\u001b[0m, in \u001b[0;36mTabPFNRegressor.predict\u001b[0;34m(self, X, output_type, quantiles)\u001b[0m\n\u001b[1;32m    621\u001b[0m outputs: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    622\u001b[0m borders: \u001b[38;5;28mlist\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor_\u001b[38;5;241m.\u001b[39miter_outputs(\n\u001b[1;32m    625\u001b[0m     X,\n\u001b[1;32m    626\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_,\n\u001b[1;32m    627\u001b[0m     autocast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_autocast_,\n\u001b[1;32m    628\u001b[0m ):\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, RegressorEnsembleConfig)\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_temperature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/inference.py:327\u001b[0m, in \u001b[0;36mInferenceEngineCachePreprocessing.iter_outputs\u001b[0;34m(self, X, device, autocast)\u001b[0m\n\u001b[1;32m    321\u001b[0m     style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    324\u001b[0m         torch\u001b[38;5;241m.\u001b[39mautocast(device\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39mautocast),\n\u001b[1;32m    325\u001b[0m         torch\u001b[38;5;241m.\u001b[39minference_mode(),\n\u001b[1;32m    326\u001b[0m     ):\n\u001b[0;32m--> 327\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;241m*\u001b[39m(style, X_full, y_train),\n\u001b[1;32m    329\u001b[0m             only_return_standard_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    330\u001b[0m             categorical_inds\u001b[38;5;241m=\u001b[39mcat_ix,\n\u001b[1;32m    331\u001b[0m             single_eval_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_train),\n\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), config\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/transformer.py:413\u001b[0m, in \u001b[0;36mPerFeatureTransformer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    412\u001b[0m     style, x, y \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x, y, style\u001b[38;5;241m=\u001b[39mstyle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized input. Please follow the doc string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/transformer.py:625\u001b[0m, in \u001b[0;36mPerFeatureTransformer._forward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere should be no NaNs in the encoded x and y.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck that you do not feed NaNs or use a NaN-handling enocder.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour embedded x and y returned the following:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39misnan(embedded_x)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39misnan(embedded_y)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    622\u001b[0m     )\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m embedded_y, embedded_x\n\u001b[0;32m--> 625\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(\n\u001b[1;32m    626\u001b[0m     (\n\u001b[1;32m    627\u001b[0m         embedded_input\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m embedded_input[:, :single_eval_pos_]\n\u001b[1;32m    630\u001b[0m     ),\n\u001b[1;32m    631\u001b[0m     single_eval_pos\u001b[38;5;241m=\u001b[39msingle_eval_pos,\n\u001b[1;32m    632\u001b[0m     half_layers\u001b[38;5;241m=\u001b[39mhalf_layers,\n\u001b[1;32m    633\u001b[0m     cache_trainset_representation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_trainset_representation,\n\u001b[1;32m    634\u001b[0m )  \u001b[38;5;66;03m# b s f+1 e -> b s f+1 e\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# If we are using a decoder\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/transformer.py:74\u001b[0m, in \u001b[0;36mLayerStack.forward\u001b[0;34m(self, x, half_layers, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint(partial(layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/layer.py:449\u001b[0m, in \u001b[0;36mPerFeatureEncoderLayer.forward\u001b[0;34m(self, state, single_eval_pos, cache_trainset_representation, att_src)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-norm implementation is wrong, as the residual should never\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be layer normed here.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m     state \u001b[38;5;241m=\u001b[39m layer_norm(\n\u001b[1;32m    444\u001b[0m         state,\n\u001b[1;32m    445\u001b[0m         allow_inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    446\u001b[0m         save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    447\u001b[0m     )\n\u001b[0;32m--> 449\u001b[0m state \u001b[38;5;241m=\u001b[39m sublayer(state)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_norm:\n\u001b[1;32m    451\u001b[0m     state \u001b[38;5;241m=\u001b[39m layer_norm(\n\u001b[1;32m    452\u001b[0m         state,\n\u001b[1;32m    453\u001b[0m         allow_inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    454\u001b[0m         save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    455\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/layer.py:363\u001b[0m, in \u001b[0;36mPerFeatureEncoderLayer.forward.<locals>.attn_between_items\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    360\u001b[0m     new_x_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_eval_pos:\n\u001b[0;32m--> 363\u001b[0m     new_x_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_between_items(\n\u001b[1;32m    364\u001b[0m         x[:, :single_eval_pos]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    365\u001b[0m         x[:, :single_eval_pos]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    366\u001b[0m         save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    367\u001b[0m         cache_kv\u001b[38;5;241m=\u001b[39mcache_trainset_representation,\n\u001b[1;32m    368\u001b[0m         only_cache_first_head_kv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m         add_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    370\u001b[0m         allow_inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    371\u001b[0m         use_cached_kv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    372\u001b[0m     )\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     new_x_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/multi_head_attention.py:355\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, x_kv, cache_kv, add_input, allow_inplace, save_peak_mem_factor, reuse_first_head_kv, only_cache_first_head_kv, use_cached_kv, use_second_set_of_queries)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k_cache \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    339\u001b[0m             batch_size,\n\u001b[1;32m    340\u001b[0m             seqlen_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_cache \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    347\u001b[0m             batch_size,\n\u001b[1;32m    348\u001b[0m             seqlen_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    353\u001b[0m         )\n\u001b[0;32m--> 355\u001b[0m output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute(\n\u001b[1;32m    356\u001b[0m     x,\n\u001b[1;32m    357\u001b[0m     x_kv,\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k_cache,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_cache,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kv_cache,\n\u001b[1;32m    361\u001b[0m     cache_kv\u001b[38;5;241m=\u001b[39mcache_kv,\n\u001b[1;32m    362\u001b[0m     use_cached_kv\u001b[38;5;241m=\u001b[39muse_cached_kv,\n\u001b[1;32m    363\u001b[0m     add_input\u001b[38;5;241m=\u001b[39madd_input,\n\u001b[1;32m    364\u001b[0m     allow_inplace\u001b[38;5;241m=\u001b[39mallow_inplace,\n\u001b[1;32m    365\u001b[0m     save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    366\u001b[0m     reuse_first_head_kv\u001b[38;5;241m=\u001b[39mreuse_first_head_kv,\n\u001b[1;32m    367\u001b[0m     use_second_set_of_queries\u001b[38;5;241m=\u001b[39muse_second_set_of_queries,\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mreshape(x_shape_after_transpose[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/memory.py:94\u001b[0m, in \u001b[0;36msupport_save_peak_mem_factor.<locals>.method_\u001b[0;34m(self, x, add_input, allow_inplace, save_peak_mem_factor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_, \u001b[38;5;241m*\u001b[39margs_ \u001b[38;5;129;01min\u001b[39;00m split_args:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m add_input:\n\u001b[0;32m---> 94\u001b[0m         x_[:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, x_, \u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         x_[:] \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, x_, \u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/multi_head_attention.py:504\u001b[0m, in \u001b[0;36mMultiHeadAttention._compute\u001b[0;34m(self, x, x_kv, k_cache, v_cache, kv_cache, cache_kv, use_cached_kv, reuse_first_head_kv, use_second_set_of_queries)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Attention computation.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03mCalled by 'forward', potentially on shards, once shapes have been normalized.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    493\u001b[0m q, k, v, kv, qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_qkv(\n\u001b[1;32m    494\u001b[0m     x,\n\u001b[1;32m    495\u001b[0m     x_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_second_set_of_queries\u001b[38;5;241m=\u001b[39muse_second_set_of_queries,\n\u001b[1;32m    503\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m attention_head_outputs \u001b[38;5;241m=\u001b[39m MultiHeadAttention\u001b[38;5;241m.\u001b[39mcompute_attention_heads(\n\u001b[1;32m    505\u001b[0m     q,\n\u001b[1;32m    506\u001b[0m     k,\n\u001b[1;32m    507\u001b[0m     v,\n\u001b[1;32m    508\u001b[0m     kv,\n\u001b[1;32m    509\u001b[0m     qkv,\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_p,\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_scale,\n\u001b[1;32m    512\u001b[0m )\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... h d, h d s -> ... s\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    515\u001b[0m     attention_head_outputs,\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w_out,\n\u001b[1;32m    517\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/tabpfn/model/multi_head_attention.py:721\u001b[0m, in \u001b[0;36mMultiHeadAttention.compute_attention_heads\u001b[0;34m(q, k, v, kv, qkv, dropout_p, softmax_scale)\u001b[0m\n\u001b[1;32m    719\u001b[0m k \u001b[38;5;241m=\u001b[39m MultiHeadAttention\u001b[38;5;241m.\u001b[39mbroadcast_kv_across_heads(k, share_kv_across_n_heads)\n\u001b[1;32m    720\u001b[0m v \u001b[38;5;241m=\u001b[39m MultiHeadAttention\u001b[38;5;241m.\u001b[39mbroadcast_kv_across_heads(v, share_kv_across_n_heads)\n\u001b[0;32m--> 721\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb q h d, b k h d -> b q k h\u001b[39m\u001b[38;5;124m\"\u001b[39m, q, k)\n\u001b[1;32m    722\u001b[0m logits \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    723\u001b[0m     torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m d_k))\u001b[38;5;241m.\u001b[39mto(k\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m softmax_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m softmax_scale\n\u001b[1;32m    726\u001b[0m )\n\u001b[1;32m    727\u001b[0m ps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/TabPFN/lib/python3.12/site-packages/torch/functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    404\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rho, beta = 0.5, 2\n",
    "n_repeats = 10\n",
    "results = []\n",
    "\n",
    "for i in range(n_repeats):\n",
    "    # split + scale\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42 + i\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # covariate shift (same as before)\n",
    "    beta1 = np.array([-1, 1, -1, 1, -1, 1, -1, 1])\n",
    "    weights = np.exp(X_test @ beta1)\n",
    "    weights /= np.sum(weights)\n",
    "    idx_shift = np.random.choice(len(X_test), size=len(X_test), replace=True, p=weights)\n",
    "    X_test_shift, y_test_shift = X_test[idx_shift], y_test[idx_shift]\n",
    "    weights_shift = weights[idx_shift]  # align weights to the shifted test set\n",
    "\n",
    "    # existing KRR-based methods\n",
    "    res = train_and_evaluate(X_train, y_train, X_test_shift, y_test_shift, rho=rho, beta=beta)\n",
    "\n",
    "    # existing: XGBoost CV (unweighted)\n",
    "    xgb_mse, xgb_best = fit_xgboost(X_train, y_train, X_test_shift, y_test_shift, random_state=42 + i)\n",
    "    res[\"xgb\"] = xgb_mse\n",
    "    res[\"xgb_best_params\"] = xgb_best\n",
    "\n",
    "    # existing: XGBoost (importance-weighted evaluation)\n",
    "    xgb_iw_mse = xgboost_importance_weighted(\n",
    "        X_train, y_train, X_test_shift, y_test_shift, weights_shift, best_params=xgb_best, random_state=42 + i\n",
    "    )\n",
    "    res[\"xgb_iw\"] = xgb_iw_mse\n",
    "\n",
    "    # NEW: TabPFN regressor (train on all X_train, evaluate on shifted test)\n",
    "    tabpfn_mse = fit_tabpfn_regressor(\n",
    "        X_train, y_train, X_test_shift, y_test_shift, random_state=42 + i, device=None  # set to 'cuda' if available\n",
    "    )\n",
    "    res[\"tabpfn\"] = tabpfn_mse\n",
    "\n",
    "    results.append(res)\n",
    "    print(f\"Iteration {i+1}/10 done. XGB best params: {xgb_best}\")\n",
    "\n",
    "# ============================================================\n",
    "# Aggregate results\n",
    "# ============================================================\n",
    "methods = [\"real\", \"pseudo\", \"naive\", \"xgb\", \"xgb_iw\", \"tabpfn\"]\n",
    "avg_results = {}\n",
    "for method in methods:\n",
    "    vals = [r[method] for r in results]\n",
    "    mean = np.mean(vals)\n",
    "    ste = np.std(vals, ddof=1) / np.sqrt(len(vals))\n",
    "    avg_results[method] = (mean, ste)\n",
    "\n",
    "print(\"\\n=== Average Results over 10 splits ===\")\n",
    "for method in methods:\n",
    "    mean, ste = avg_results[method]\n",
    "    print(f\"{method.capitalize():<8}: {mean:.4f} ± {ste:.4f}\")\n",
    "\n",
    "# (Optional) average selected λ for KRR-based selectors\n",
    "lbd_naive_mean = np.mean([r[\"lbd_naive\"] for r in results])\n",
    "lbd_pseudo_mean = np.mean([r[\"lbd_pseudo\"] for r in results])\n",
    "lbd_real_mean = np.mean([r[\"lbd_real\"] for r in results])\n",
    "print(\"\\nAverage selected λ (KRR):\")\n",
    "print(f\"λ* Naive : {lbd_naive_mean:.4g}\")\n",
    "print(f\"λ* Pseudo: {lbd_pseudo_mean:.4g}\")\n",
    "print(f\"λ* Real  : {lbd_real_mean:.4g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TabPFN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
