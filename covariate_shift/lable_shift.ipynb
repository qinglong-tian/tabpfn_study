{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b88ffacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabpfn import TabPFNClassifier  # assume installed\n",
    "\n",
    "rng = default_rng(42)\n",
    "\n",
    "def make_cov(d=10, rho=0.5, sigma2=1.5):\n",
    "    \"\"\"Î£ = sigma2 * [(1-rho) I + rho * 11^T]\"\"\"\n",
    "    I = np.eye(d)\n",
    "    J = np.ones((d, d))\n",
    "    return sigma2 * ((1 - rho) * I + rho * J)\n",
    "\n",
    "def make_label_shift_data(n_train=4000, n_test=4000, d=12,\n",
    "                          train_prior=0.50, test_prior=0.30,\n",
    "                          rho=0.6, sigma2=2.0, target_mahal=0.8,\n",
    "                          random_orientation=True, seed=42):\n",
    "    \"\"\"\n",
    "    Generate train/test from two d-dim Gaussians with DIFFERENT PRIORS (label shift),\n",
    "    and control overlap via the Mahalanobis distance between class means.\n",
    "\n",
    "    Returns:\n",
    "      (Xtr, ytr), (Xte, yte), params (dict with mu0, mu1, Sigma, priors, etc.)\n",
    "    \"\"\"\n",
    "    rng_local = default_rng(seed)\n",
    "    Sigma = make_cov(d, rho=rho, sigma2=sigma2)\n",
    "\n",
    "    mu0 = np.zeros(d)\n",
    "    if random_orientation:\n",
    "        u = rng_local.normal(size=d)\n",
    "        u = u / np.linalg.norm(u)\n",
    "    else:\n",
    "        u = np.ones(d) / np.sqrt(d)\n",
    "\n",
    "    # Choose mu1 so that (mu1 - mu0)^T Sigma^{-1} (mu1 - mu0) = target_mahal^2\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    denom = np.sqrt(u @ Sigma_inv @ u)\n",
    "    s = (target_mahal / denom) if denom > 0 else 0.0\n",
    "    mu1 = s * u\n",
    "\n",
    "    def sample(n, p1):\n",
    "        y = rng_local.binomial(1, p1, size=n)\n",
    "        x = np.empty((n, d))\n",
    "        idx1 = np.where(y == 1)[0]\n",
    "        idx0 = np.where(y == 0)[0]\n",
    "        if len(idx1) > 0:\n",
    "            x[idx1] = rng_local.multivariate_normal(mu1, Sigma, size=len(idx1))\n",
    "        if len(idx0) > 0:\n",
    "            x[idx0] = rng_local.multivariate_normal(mu0, Sigma, size=len(idx0))\n",
    "        return x, y\n",
    "\n",
    "    Xtr, ytr = sample(n_train, train_prior)\n",
    "    Xte, yte = sample(n_test, test_prior)\n",
    "\n",
    "    params_used = dict(\n",
    "        d=d, rho=rho, sigma2=sigma2, target_mahal=target_mahal,\n",
    "        train_prior=train_prior, test_prior=test_prior,\n",
    "        mu0=mu0, mu1=mu1, Sigma=Sigma\n",
    "    )\n",
    "    return (Xtr, ytr), (Xte, yte), params_used\n",
    "\n",
    "def sample_from_gaussians(n, prior1, mu0, mu1, Sigma, seed=None):\n",
    "    \"\"\"Sample n points with class-1 prior `prior1` from N(mu0,Sigma)/N(mu1,Sigma).\"\"\"\n",
    "    rng_local = default_rng(seed)\n",
    "    d = len(mu0)\n",
    "    y = rng_local.binomial(1, prior1, size=n)\n",
    "    x = np.empty((n, d))\n",
    "    idx1 = np.where(y == 1)[0]\n",
    "    idx0 = np.where(y == 0)[0]\n",
    "    if len(idx1) > 0:\n",
    "        x[idx1] = rng_local.multivariate_normal(mu1, Sigma, size=len(idx1))\n",
    "    if len(idx0) > 0:\n",
    "        x[idx0] = rng_local.multivariate_normal(mu0, Sigma, size=len(idx0))\n",
    "    return x, y\n",
    "\n",
    "def fit_logreg(X_train, y_train, X_test):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    Xtr_s = scaler.transform(X_train)\n",
    "    Xte_s = scaler.transform(X_test)\n",
    "    clf = LogisticRegression(max_iter=500, solver=\"lbfgs\")\n",
    "    clf.fit(Xtr_s, y_train)\n",
    "    p_te = clf.predict_proba(Xte_s)[:, 1]\n",
    "    return p_te, clf, scaler  # probabilities + model + scaler\n",
    "\n",
    "def evaluate(y_true, p1):\n",
    "    yhat = (p1 >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, yhat)\n",
    "    auc = roc_auc_score(y_true, p1)\n",
    "    return acc, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf951d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data params: {'d': 12, 'rho': 0.6, 'sigma2': 2.0, 'target_mahal': 0.8, 'train_prior': 0.5, 'test_prior': 0.3}\n",
      "LogReg (no correction):        ACC=0.645, AUC=0.699\n",
      "LogReg + BBSE (corrected):     ACC=0.708, AUC=0.699\n",
      "Oracle LogReg (upper bound):   ACC=0.723, AUC=0.700\n",
      "TabPFN (plain, CUDA):          ACC=0.641, AUC=0.698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---- Black-Box Shift Estimation (Lipton et al., 2018) ----\n",
    "def bbse_adjusted_probs(clf, scaler, X_train, y_train, X_test, n_calib=0.2, n_classes=2):\n",
    "    \"\"\"\n",
    "    Return BBSE-adjusted probabilities on X_test.\n",
    "    We *do not* return/print the estimated prior; only adjusted probs for accuracy/AUC.\n",
    "    \"\"\"\n",
    "    # 1) Build confusion matrix C on a calibration split from TRAIN\n",
    "    X_tr_cal, _, y_tr_cal, _ = train_test_split(\n",
    "        X_train, y_train, test_size=(1 - n_calib), stratify=y_train, random_state=123\n",
    "    )\n",
    "    X_cal_s = scaler.transform(X_tr_cal)\n",
    "    yhat_cal = clf.predict(X_cal_s)\n",
    "\n",
    "    C = np.zeros((n_classes, n_classes))  # rows: predicted, cols: true\n",
    "    for t in range(n_classes):\n",
    "        idx = (y_tr_cal == t)\n",
    "        if idx.sum() > 0:\n",
    "            counts = np.bincount(yhat_cal[idx], minlength=n_classes)\n",
    "            C[:, t] = counts / idx.sum()\n",
    "\n",
    "    # 2) Predicted-label distribution on unlabeled TEST\n",
    "    Xte_s = scaler.transform(X_test)\n",
    "    yhat_te = clf.predict(Xte_s)\n",
    "    q_hat = np.bincount(yhat_te, minlength=n_classes) / len(yhat_te)\n",
    "\n",
    "    # 3) Estimate test priors internally (no printing), then correct posteriors\n",
    "    pi_t_hat = np.linalg.pinv(C) @ q_hat\n",
    "    pi_t_hat = np.clip(pi_t_hat, 1e-6, 1.0)\n",
    "    pi_t_hat = pi_t_hat / pi_t_hat.sum()\n",
    "\n",
    "    pi_s = np.bincount(y_train, minlength=n_classes) / len(y_train)\n",
    "\n",
    "    # Source posteriors from LR\n",
    "    p_src = clf.predict_proba(Xte_s)  # shape (n, 2)\n",
    "    ratios = (pi_t_hat / np.maximum(pi_s, 1e-12))[None, :]\n",
    "    p_adj = p_src * ratios\n",
    "    p_adj = p_adj / np.maximum(p_adj.sum(axis=1, keepdims=True), 1e-12)\n",
    "    return p_adj[:, 1]  # adjusted P(Y=1|x) for test points\n",
    "\n",
    "# ---------------------- Configure a HARD scenario ----------------------\n",
    "(Xtr, ytr), (Xte, yte), pars = make_label_shift_data(\n",
    "    n_train=4000, n_test=4000,\n",
    "    d=12,\n",
    "    train_prior=0.50, test_prior=0.30,\n",
    "    rho=0.6,\n",
    "    sigma2=2.0,\n",
    "    target_mahal=0.8,\n",
    "    random_orientation=True,\n",
    "    seed=7\n",
    ")\n",
    "\n",
    "print(\"Data params:\",\n",
    "      {k: v for k, v in pars.items() if k not in (\"mu0\", \"mu1\", \"Sigma\")})\n",
    "\n",
    "# ---- Baseline Logistic Regression (trained on TRAIN distribution) ----\n",
    "p_lr, lr_clf, lr_scaler = fit_logreg(Xtr, ytr, Xte)\n",
    "acc_lr, auc_lr = evaluate(yte, p_lr)\n",
    "print(f\"LogReg (no correction):        ACC={acc_lr:.3f}, AUC={auc_lr:.3f}\")\n",
    "\n",
    "# ---- Logistic + BBSE (accuracy/AUC only; no prior output) ----\n",
    "p_lr_bbse = bbse_adjusted_probs(lr_clf, lr_scaler, Xtr, ytr, Xte, n_calib=0.2)\n",
    "acc_lr_bbse, auc_lr_bbse = evaluate(yte, p_lr_bbse)\n",
    "print(f\"LogReg + BBSE (corrected):     ACC={acc_lr_bbse:.3f}, AUC={auc_lr_bbse:.3f}\")\n",
    "\n",
    "# ---- Oracle Logistic Regression (upper bound for LR) ----\n",
    "X_oracle, y_oracle = sample_from_gaussians(\n",
    "    n=len(ytr),\n",
    "    prior1=pars[\"test_prior\"],\n",
    "    mu0=pars[\"mu0\"], mu1=pars[\"mu1\"], Sigma=pars[\"Sigma\"],\n",
    "    seed=999\n",
    ")\n",
    "p_oracle, _, _ = fit_logreg(X_oracle, y_oracle, Xte)\n",
    "acc_oracle, auc_oracle = evaluate(yte, p_oracle)\n",
    "print(f\"Oracle LogReg (upper bound):   ACC={acc_oracle:.3f}, AUC={auc_oracle:.3f}\")\n",
    "\n",
    "# ---- TabPFN (trained on TRAIN distribution, CUDA) ----\n",
    "scaler_t = StandardScaler().fit(Xtr)\n",
    "Xtr_s = scaler_t.transform(Xtr).astype(\"float32\")\n",
    "Xte_s = scaler_t.transform(Xte).astype(\"float32\")\n",
    "\n",
    "tab = TabPFNClassifier(device=\"cuda\")\n",
    "tab.fit(Xtr_s, ytr.astype(int))\n",
    "p_tab = tab.predict_proba(Xte_s)[:, 1]\n",
    "acc_tab, auc_tab = evaluate(yte, p_tab)\n",
    "print(f\"TabPFN (plain, CUDA):          ACC={acc_tab:.3f}, AUC={auc_tab:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
